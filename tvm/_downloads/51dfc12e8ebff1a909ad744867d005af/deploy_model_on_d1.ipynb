{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nDeploy the test Model on D1\n===========================\n\nThis is an example of using Relay to deploy mobilenet on Raspberry Pi.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nimport tvm.relay as relay\nfrom tvm import rpc\nimport tvm.relay.testing\nfrom tvm.contrib import utils, graph_executor as runtime\nfrom tvm.relay.op.contrib import shl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build TVM Runtime on Device\n---------------------------\n\nThe first step is to build the TVM runtime on the remote device.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>All instructions in both this section and next section should be\n  executed on the target device, e.g. Raspberry Pi. And we assume it\n  has Linux running.</p></div>\n\nSince we do compilation on local machine, the remote device is only used\nfor running the generated code. We only need to build tvm runtime on\nthe remote device.\n\n.. code-block:: bash\n\n  mkdir build\n  cp cmake/config.cmake build\n  cd build\n  cmake ..\n  make runtime -j4 tvm_rpc\n\nAfter building runtime successfully, we need to copy tvm_rpc and libs\nwhich used on D1\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Up RPC Server on Device\n---------------------------\nTo start an RPC server, run the following command on your remote device\n\n  .. code-block:: bash\n\n    ./tvm_rpc server --host=172.16.202.11 --port=9090\n\nIf you see the line below, it means the RPC server started\nsuccessfully on your device.\n\n   .. code-block:: bash\n\n     rpc_server.cc:130: bind to 172.16.202.11:9090\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a Network\n----------------\nFirst, we need to define the network with relay frontend API.\nWe can load some pre-defined network from :code:`tvm.relay.testing`.\nWe can also load models from MXNet, ONNX, PyTorch, and TensorFlow\n(see `front end tutorials<tutorial-frontend>`).\n\nFor convolutional neural networks, although auto-scheduler can work correctly\nwith any layout, we found the best performance is typically achieved with NHWC layout.\nWe also implemented more optimizations for NHWC layout with the auto-scheduler.\nSo it is recommended to convert your models to NHWC layout to use the auto-scheduler.\nYou can use `ConvertLayout <convert-layout-usage>` pass to do the layout conversion in TVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_network(name, batch_size, layout=\"NHWC\", dtype=\"float32\", use_sparse=False):\n    \"\"\"Get the symbol definition and random weight of a network\"\"\"\n\n    # auto-scheduler prefers NHWC layout\n    if layout == \"NHWC\":\n        image_shape = (224, 224, 3)\n    elif layout == \"NCHW\":\n        image_shape = (3, 224, 224)\n    else:\n        raise ValueError(\"Invalid layout: \" + layout)\n\n    input_shape = (batch_size,) + image_shape\n    output_shape = (batch_size, 1000)\n\n    if name.startswith(\"resnet-\"):\n        n_layer = int(name.split(\"-\")[1])\n        mod, params = relay.testing.resnet.get_workload(\n            num_layers=n_layer,\n            batch_size=batch_size,\n            layout=layout,\n            dtype=dtype,\n            image_shape=image_shape,\n        )\n    elif name.startswith(\"resnet3d-\"):\n        n_layer = int(name.split(\"-\")[1])\n        mod, params = relay.testing.resnet.get_workload(\n            num_layers=n_layer,\n            batch_size=batch_size,\n            layout=layout,\n            dtype=dtype,\n            image_shape=image_shape,\n        )\n    elif name == \"mobilenet\":\n        mod, params = relay.testing.mobilenet.get_workload(\n            batch_size=batch_size, layout=layout, dtype=dtype, image_shape=image_shape\n        )\n    elif name == \"squeezenet_v1.1\":\n        assert layout == \"NCHW\", \"squeezenet_v1.1 only supports NCHW layout\"\n        mod, params = relay.testing.squeezenet.get_workload(\n            version=\"1.1\",\n            batch_size=batch_size,\n            dtype=dtype,\n            image_shape=image_shape,\n        )\n    elif name == \"inception_v3\":\n        input_shape = (batch_size, 3, 299, 299) if layout == \"NCHW\" else (batch_size, 299, 299, 3)\n        mod, params = relay.testing.inception_v3.get_workload(batch_size=batch_size, dtype=dtype)\n    elif name == \"mxnet\":\n        # an example for mxnet model\n        from mxnet.gluon.model_zoo.vision import get_model\n\n        assert layout == \"NCHW\"\n\n        block = get_model(\"resnet50_v1\", pretrained=True)\n        mod, params = relay.frontend.from_mxnet(block, shape={\"data\": input_shape}, dtype=dtype)\n        net = mod[\"main\"]\n        net = relay.Function(\n            net.params, relay.nn.softmax(net.body), None, net.type_params, net.attrs\n        )\n        mod = tvm.IRModule.from_expr(net)\n    elif name == \"mlp\":\n        mod, params = relay.testing.mlp.get_workload(\n            batch_size=batch_size, dtype=dtype, image_shape=image_shape, num_classes=1000\n        )\n    else:\n        raise ValueError(\"Network not found.\")\n\n    if use_sparse:\n        from tvm.topi.sparse.utils import convert_model_dense_to_sparse\n\n        mod, params = convert_model_dense_to_sparse(mod, params, random_params=True)\n\n    return mod, params, input_shape, output_shape\n\n\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile The Graph\n-----------------\nTo compile the graph, we call the :py:func:`relay.build` function\nwith the graph configuration and parameters. However, You cannot to\ndeploy a x86 program on a device with RISC-V instruction set. It means\nRelay also needs to know the compilation option of target device,\napart from arguments :code:`net` and :code:`params` to specify the\ndeep learning workload. Actually, the option matters, different option\nwill lead to very different performance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we run the example on our x86 server for demonstration, we can simply\nset it as :code:`llvm`. If running it on the Raspberry Pi, we need to\nspecify its instruction set. Set :code:`local_demo` to False if you want\nto run this tutorial with a real device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "local_demo = False\n\nif local_demo:\n    target = tvm.target.Target(\"llvm\")\nelse:\n    target = tvm.target.Target(\n        \"llvm -mtriple=riscv64-unknown-linux-gnu -mcpu=sifive-u74 -mabi=lp64d\"\n    )\n\nnetwork = \"mobilenet\"\nuse_sparse = False\nbatch_size = 1\nlayout = \"NCHW\"\ndtype = \"float32\"\nlog_file = \"%s-%s-B%d-%s.json\" % (network, layout, batch_size, target.kind.name)\nprint(\"Get model...\")\nmod, params, input_shape, output_shape = get_network(\n    network, batch_size, layout, dtype=dtype, use_sparse=use_sparse\n)\n\nwith tvm.transform.PassContext(opt_level=3):\n    mod = shl.partition_for_shl(mod, params)\n\n    lib = relay.build(mod, target, params=params)\n\n# After `relay.build`, you will get three return values: graph,\n# library and the new parameter, since we do some optimization that will\n# change the parameters but keep the result of model as the same.\n\n# Save the library at local temporary directory.\ntmp = utils.tempdir()\nlib_fname = tmp.relpath(\"net.so\")\nif local_demo:\n    lib.export_library(lib_fname)\nelse:\n    lib.export_library(lib_fname, cc=\"riscv64-unknown-linux-gnu-g++\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy the Model Remotely by RPC\n--------------------------------\nWith RPC, you can deploy the model remotely from your host machine\nto the remote device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# obtain an RPC session from remote device.\nif local_demo:\n    remote = rpc.LocalSession()\nelse:\n    # The following is my environment, change this to the IP address of your target device\n    host = \"127.0.0.1\"\n    port = 9090\n    remote = rpc.connect(host, port)\n\n# upload the library to remote device and load it\nremote.upload(lib_fname)\nrlib = remote.load_module(\"net.so\")\n\n# create the remote runtime module\ndev = remote.cpu(0)\nmodule = runtime.GraphModule(rlib[\"default\"](dev))\n# set input data\ndata_tvm = tvm.nd.array((np.random.uniform(size=input_shape)).astype(dtype))\nmodule.set_input(\"data\", data_tvm)\n# # run\nmodule.run()\n# get output\nout = module.get_output(0)\n# get top1 result\ntop1 = np.argmax(out.numpy())\nprint(\"TVM prediction top-1: {}\".format(top1))\n\n# print(\"Evaluate inference time cost...\")\n# print(module.benchmark(dev, repeat=1, number=1, min_repeat_ms=500))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}